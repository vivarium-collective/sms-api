{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"sms-api","text":"<p>This is the api server for Vivarium simulation services.</p>"},{"location":"AWS_S3_SETUP/","title":"AWS S3 Setup for SMS API","text":"<p>This document describes how to set up an AWS S3 bucket for use with the SMS API, including proper permissions for SSO users and avoiding KMS encryption issues.</p>"},{"location":"AWS_S3_SETUP/#requirements","title":"Requirements","text":""},{"location":"AWS_S3_SETUP/#s3-bucket-configuration","title":"S3 Bucket Configuration","text":"<ul> <li>No KMS encryption (or proper KMS key permissions for your SSO role)</li> <li>Standard S3 bucket in your preferred region</li> <li>Versioning (optional but recommended)</li> <li>Lifecycle policies (optional, for automatic cleanup)</li> </ul>"},{"location":"AWS_S3_SETUP/#iam-permissions-required","title":"IAM Permissions Required","text":"<p>Your SSO role needs the following S3 permissions: - <code>s3:PutObject</code> - Upload files - <code>s3:GetObject</code> - Download files (also covers head/metadata operations) - <code>s3:DeleteObject</code> - Delete files - <code>s3:ListBucket</code> - List bucket contents - <code>s3:GetObjectAttributes</code> - Get object metadata</p> <p>If using KMS encryption, you also need: - <code>kms:GenerateDataKey</code> - Generate encryption keys for new objects - <code>kms:Decrypt</code> - Decrypt existing objects</p>"},{"location":"AWS_S3_SETUP/#setup-instructions","title":"Setup Instructions","text":""},{"location":"AWS_S3_SETUP/#step-1-check-current-bucket-configuration","title":"Step 1: Check Current Bucket Configuration","text":"<pre><code># Check if bucket exists\naws s3 ls s3://your-bucket-name/\n\n# Check bucket encryption\naws s3api get-bucket-encryption --bucket your-bucket-name\n\n# Check your current identity\naws sts get-caller-identity\n</code></pre>"},{"location":"AWS_S3_SETUP/#step-2-create-a-new-bucket-without-kms","title":"Step 2: Create a New Bucket (Without KMS)","text":"<pre><code># Set variables\nexport AWS_REGION=\"us-east-1\"\nexport BUCKET_NAME=\"sms-api-storage-$(date +%Y%m%d)\"\nexport ACCOUNT_ID=$(aws sts get-caller-identity --query Account --output text)\nexport SSO_ROLE_ARN=\"arn:aws:sts::${ACCOUNT_ID}:assumed-role/AWSReservedSSO_AdministratorAccess_0623eedd7adb8854/jcschaff_sso\"\n\n# Create bucket\naws s3api create-bucket \\\n  --bucket \"$BUCKET_NAME\" \\\n  --region \"$AWS_REGION\" \\\n  --create-bucket-configuration LocationConstraint=\"$AWS_REGION\"\n\necho \"\u2705 Created bucket: $BUCKET_NAME\"\n</code></pre> <p>Note: For <code>us-east-1</code>, omit the <code>--create-bucket-configuration</code> parameter:</p> <pre><code>aws s3api create-bucket \\\n  --bucket \"$BUCKET_NAME\" \\\n  --region us-east-1\n</code></pre>"},{"location":"AWS_S3_SETUP/#step-3-configure-bucket-encryption-sse-s3-not-kms","title":"Step 3: Configure Bucket Encryption (SSE-S3, not KMS)","text":"<p>Use S3-managed encryption (SSE-S3) instead of KMS to avoid permission issues:</p> <pre><code>aws s3api put-bucket-encryption \\\n  --bucket \"$BUCKET_NAME\" \\\n  --server-side-encryption-configuration '{\n    \"Rules\": [\n      {\n        \"ApplyServerSideEncryptionByDefault\": {\n          \"SSEAlgorithm\": \"AES256\"\n        },\n        \"BucketKeyEnabled\": false\n      }\n    ]\n  }'\n\necho \"\u2705 Configured SSE-S3 encryption (no KMS required)\"\n</code></pre>"},{"location":"AWS_S3_SETUP/#step-4-enable-versioning-optional-but-recommended","title":"Step 4: Enable Versioning (Optional but Recommended)","text":"<pre><code>aws s3api put-bucket-versioning \\\n  --bucket \"$BUCKET_NAME\" \\\n  --versioning-configuration Status=Enabled\n\necho \"\u2705 Enabled versioning\"\n</code></pre>"},{"location":"AWS_S3_SETUP/#step-5-set-bucket-policy-for-sso-access","title":"Step 5: Set Bucket Policy for SSO Access","text":"<p>Create a bucket policy that explicitly allows your SSO role:</p> <pre><code>cat &gt; /tmp/bucket-policy.json &lt;&lt;EOF\n{\n  \"Version\": \"2012-10-17\",\n  \"Statement\": [\n    {\n      \"Sid\": \"AllowSSOUserFullAccess\",\n      \"Effect\": \"Allow\",\n      \"Principal\": {\n        \"AWS\": \"${SSO_ROLE_ARN}\"\n      },\n      \"Action\": [\n        \"s3:PutObject\",\n        \"s3:GetObject\",\n        \"s3:DeleteObject\",\n        \"s3:ListBucket\",\n        \"s3:GetObjectAttributes\"\n      ],\n      \"Resource\": [\n        \"arn:aws:s3:::${BUCKET_NAME}\",\n        \"arn:aws:s3:::${BUCKET_NAME}/*\"\n      ]\n    }\n  ]\n}\nEOF\n\naws s3api put-bucket-policy \\\n  --bucket \"$BUCKET_NAME\" \\\n  --policy file:///tmp/bucket-policy.json\n\necho \"\u2705 Set bucket policy for SSO access\"\n</code></pre>"},{"location":"AWS_S3_SETUP/#step-6-configure-lifecycle-policy-optional","title":"Step 6: Configure Lifecycle Policy (Optional)","text":"<p>Automatically delete test files after 7 days:</p> <pre><code>cat &gt; /tmp/lifecycle-policy.json &lt;&lt;EOF\n{\n  \"Rules\": [\n    {\n      \"Id\": \"DeleteTestFilesAfter7Days\",\n      \"Status\": \"Enabled\",\n      \"Prefix\": \"test/\",\n      \"Expiration\": {\n        \"Days\": 7\n      }\n    }\n  ]\n}\nEOF\n\naws s3api put-bucket-lifecycle-configuration \\\n  --bucket \"$BUCKET_NAME\" \\\n  --lifecycle-configuration file:///tmp/lifecycle-policy.json\n\necho \"\u2705 Configured lifecycle policy\"\n</code></pre>"},{"location":"AWS_S3_SETUP/#step-7-test-bucket-access","title":"Step 7: Test Bucket Access","text":"<pre><code># Test upload\necho \"Test content\" &gt; /tmp/test-file.txt\naws s3 cp /tmp/test-file.txt \"s3://${BUCKET_NAME}/test/upload-test.txt\"\n\n# Test download\naws s3 cp \"s3://${BUCKET_NAME}/test/upload-test.txt\" /tmp/test-download.txt\n\n# Test delete\naws s3 rm \"s3://${BUCKET_NAME}/test/upload-test.txt\"\n\n# Cleanup\nrm /tmp/test-file.txt /tmp/test-download.txt\n\necho \"\u2705 All operations succeeded!\"\n</code></pre>"},{"location":"AWS_S3_SETUP/#step-8-update-environment-configuration","title":"Step 8: Update Environment Configuration","text":"<p>Add to your <code>.env</code> file or environment:</p> <pre><code>export STORAGE_S3_BUCKET=\"${BUCKET_NAME}\"\nexport STORAGE_S3_REGION=\"${AWS_REGION}\"\n# AWS credentials are typically auto-configured via SSO\n</code></pre>"},{"location":"AWS_S3_SETUP/#troubleshooting","title":"Troubleshooting","text":""},{"location":"AWS_S3_SETUP/#error-kmsgeneratedatakey-permission-denied","title":"Error: <code>kms:GenerateDataKey</code> Permission Denied","text":"<p>This means your bucket has KMS encryption enabled but your SSO role lacks KMS permissions.</p> <p>Solution 1: Use SSE-S3 instead (recommended, see Step 3 above)</p> <p>Solution 2: Add KMS permissions to your role</p> <pre><code># Check the KMS key being used\naws s3api get-bucket-encryption --bucket \"$BUCKET_NAME\"\n\n# Your AWS administrator needs to add this to your SSO role policy:\n{\n  \"Effect\": \"Allow\",\n  \"Action\": [\n    \"kms:GenerateDataKey\",\n    \"kms:Decrypt\"\n  ],\n  \"Resource\": \"arn:aws:kms:REGION:ACCOUNT:key/KEY-ID\"\n}\n</code></pre>"},{"location":"AWS_S3_SETUP/#error-accessdenied-on-upload","title":"Error: <code>AccessDenied</code> on Upload","text":"<p>Check bucket policy and IAM permissions:</p> <pre><code># View current bucket policy\naws s3api get-bucket-policy --bucket \"$BUCKET_NAME\" | jq -r '.Policy | fromjson'\n\n# Check your identity\naws sts get-caller-identity\n</code></pre>"},{"location":"AWS_S3_SETUP/#error-bucket-already-exists","title":"Error: Bucket Already Exists","text":"<p>Bucket names are globally unique. Choose a different name:</p> <pre><code>export BUCKET_NAME=\"sms-api-storage-$(uuidgen | head -c 8 | tr '[:upper:]' '[:lower:]')\"\n</code></pre>"},{"location":"AWS_S3_SETUP/#common-aws-cli-operations","title":"Common AWS CLI Operations","text":""},{"location":"AWS_S3_SETUP/#upload-file","title":"Upload File","text":"<pre><code>aws s3 cp local-file.txt s3://${BUCKET_NAME}/path/to/file.txt\n</code></pre>"},{"location":"AWS_S3_SETUP/#upload-with-metadata","title":"Upload with Metadata","text":"<pre><code>aws s3 cp local-file.txt s3://${BUCKET_NAME}/path/to/file.txt \\\n  --metadata key1=value1,key2=value2\n</code></pre>"},{"location":"AWS_S3_SETUP/#download-file","title":"Download File","text":"<pre><code>aws s3 cp s3://${BUCKET_NAME}/path/to/file.txt local-file.txt\n</code></pre>"},{"location":"AWS_S3_SETUP/#list-files","title":"List Files","text":"<pre><code># List all files\naws s3 ls s3://${BUCKET_NAME}/ --recursive\n\n# List files with prefix\naws s3 ls s3://${BUCKET_NAME}/test/ --recursive\n</code></pre>"},{"location":"AWS_S3_SETUP/#delete-file","title":"Delete File","text":"<pre><code>aws s3 rm s3://${BUCKET_NAME}/path/to/file.txt\n</code></pre>"},{"location":"AWS_S3_SETUP/#delete-all-files-in-prefix","title":"Delete All Files in Prefix","text":"<pre><code>aws s3 rm s3://${BUCKET_NAME}/test/ --recursive\n</code></pre>"},{"location":"AWS_S3_SETUP/#sync-directory-to-s3","title":"Sync Directory to S3","text":"<pre><code>aws s3 sync local-directory/ s3://${BUCKET_NAME}/remote-directory/\n</code></pre>"},{"location":"AWS_S3_SETUP/#get-object-metadata","title":"Get Object Metadata","text":"<pre><code>aws s3api head-object \\\n  --bucket \"$BUCKET_NAME\" \\\n  --key path/to/file.txt\n</code></pre>"},{"location":"AWS_S3_SETUP/#check-if-object-exists","title":"Check if Object Exists","text":"<pre><code>if aws s3api head-object --bucket \"$BUCKET_NAME\" --key path/to/file.txt 2&gt;/dev/null; then\n  echo \"File exists\"\nelse\n  echo \"File does not exist\"\nfi\n</code></pre>"},{"location":"AWS_S3_SETUP/#python-api-usage","title":"Python API Usage","text":""},{"location":"AWS_S3_SETUP/#using-fileservices3","title":"Using FileServiceS3","text":"<pre><code>from sms_api.common.storage import FileServiceS3\n\n# Initialize service (uses credentials from environment)\ns3_service = FileServiceS3()\n\n# Upload bytes\nawait s3_service.upload_bytes(\n    file_contents=b\"Hello, world!\",\n    gcs_path=\"test/hello.txt\"\n)\n\n# Upload file\nfrom pathlib import Path\nawait s3_service.upload_file(\n    file_path=Path(\"local-file.txt\"),\n    gcs_path=\"test/uploaded-file.txt\"\n)\n\n# Download file\ngcs_path, local_path = await s3_service.download_file(\n    gcs_path=\"test/hello.txt\",\n    file_path=Path(\"downloaded-file.txt\")\n)\n\n# Get file contents\ncontents = await s3_service.get_file_contents(\"test/hello.txt\")\nprint(contents.decode(\"utf-8\"))\n\n# List files\nfrom sms_api.common.storage import ListingItem\nfiles: list[ListingItem] = await s3_service.get_listing(\"test/\")\nfor file in files:\n    print(f\"{file.Key} - {file.Size} bytes - {file.LastModified}\")\n\n# Delete file\nawait s3_service.delete_file(\"test/hello.txt\")\n\n# Close service\nawait s3_service.close()\n</code></pre>"},{"location":"AWS_S3_SETUP/#security-best-practices","title":"Security Best Practices","text":"<ol> <li>Use SSE-S3 encryption instead of KMS unless you have specific compliance requirements</li> <li>Enable bucket versioning to protect against accidental deletions</li> <li>Use lifecycle policies to automatically clean up old test files</li> <li>Use IAM roles (SSO or EC2 instance roles) instead of hardcoded credentials</li> <li>Enable CloudTrail logging for audit trails</li> <li>Use bucket policies to restrict access to specific principals</li> <li>Enable S3 Block Public Access to prevent accidental public exposure</li> </ol>"},{"location":"AWS_S3_SETUP/#configuration-in-sms-api","title":"Configuration in SMS API","text":""},{"location":"AWS_S3_SETUP/#environment-variables","title":"Environment Variables","text":"<pre><code># Required\nexport STORAGE_S3_BUCKET=\"your-bucket-name\"\nexport STORAGE_S3_REGION=\"us-east-1\"\n\n# Optional (if not using SSO/instance role)\nexport STORAGE_S3_ACCESS_KEY_ID=\"AKIAIOSFODNN7EXAMPLE\"\nexport STORAGE_S3_SECRET_ACCESS_KEY=\"wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY\"\nexport STORAGE_S3_SESSION_TOKEN=\"mysessiontoken\"\n</code></pre>"},{"location":"AWS_S3_SETUP/#settings-configpy","title":"Settings (config.py)","text":"<pre><code>from sms_api.config import get_settings\n\nsettings = get_settings()\nprint(f\"S3 Bucket: {settings.storage_s3_bucket}\")\nprint(f\"S3 Region: {settings.storage_s3_region}\")\n</code></pre>"},{"location":"AWS_S3_SETUP/#comparison-with-qumulo","title":"Comparison with Qumulo","text":"Feature AWS S3 Qumulo S3 Overwrites \u2705 Allowed by default \u274c Blocked (requires delete-first) Encryption SSE-S3, SSE-KMS, SSE-C Compatible with S3 API Endpoint Standard AWS endpoints Custom endpoint URL Authentication AWS credentials/SSO S3-compatible keys Checksums Full AWS checksum support Limited (when_required mode) SSL Required (standard) Optional (--no-verify-ssl)"},{"location":"AWS_S3_SETUP/#references","title":"References","text":"<ul> <li>AWS S3 API Documentation</li> <li>AWS CLI S3 Commands</li> <li>AWS S3 Encryption</li> <li>AWS S3 IAM Policies</li> </ul>"},{"location":"QUMULO_SETUP/","title":"Qumulo S3 Storage Setup Guide","text":""},{"location":"QUMULO_SETUP/#overview","title":"Overview","text":"<p>This guide explains how to configure and test the Qumulo S3-compatible FileService implementation.</p>"},{"location":"QUMULO_SETUP/#network-requirements","title":"Network Requirements","text":"<p>\u26a0\ufe0f Important: The Qumulo server (<code>cfs15.cam.uchc.edu:9000</code>) is on a private network and requires:</p> <ul> <li>VPN connection to the UCHC network, OR</li> <li>Direct connection to the campus network</li> </ul>"},{"location":"QUMULO_SETUP/#configuration","title":"Configuration","text":"<p>The Qumulo settings are already configured in <code>assets/dev/config/.dev_env</code>:</p> <pre><code># Qumulo S3-compatible storage settings\nSTORAGE_QUMULO_ENDPOINT_URL=https://cfs15.cam.uchc.edu:9000\nSTORAGE_QUMULO_BUCKET=sms-vivarium\nSTORAGE_QUMULO_ACCESS_KEY_ID=&lt;&lt;access-key-id&gt;&gt;\nSTORAGE_QUMULO_SECRET_ACCESS_KEY=&lt;&lt;secret-access-key&gt;&gt;\nSTORAGE_QUMULO_VERIFY_SSL=false\n</code></pre>"},{"location":"QUMULO_SETUP/#key-configuration-notes","title":"Key Configuration Notes:","text":"<ol> <li>Endpoint URL: <code>https://cfs15.cam.uchc.edu:9000</code></li> <li>Uses HTTPS (not HTTP) - the server requires TLS/SSL</li> <li>Port 9000 is the standard S3-compatible API port</li> <li> <p>Path-based bucket access</p> </li> <li> <p>Bucket: <code>sms-vivarium</code></p> </li> <li>Path-based (not virtual-hosted style)</li> <li> <p>Filesystem-oriented storage</p> </li> <li> <p>SSL Verification: <code>false</code></p> </li> <li>Set to false because the server uses a self-signed certificate</li> <li>The connection is encrypted (HTTPS) but certificate verification is disabled</li> <li> <p>In production, you may want to configure proper SSL certificates</p> </li> <li> <p>Checksum Compatibility: Qumulo doesn't support AWS's newer checksums</p> </li> <li>The FileService automatically disables CRC64NVME and other newer checksums</li> <li>Sets <code>AWS_REQUEST_CHECKSUM_CALCULATION=when_required</code> environment variable</li> <li>This is required for write operations to succeed</li> </ol>"},{"location":"QUMULO_SETUP/#testing-qumulo-connection","title":"Testing Qumulo Connection","text":""},{"location":"QUMULO_SETUP/#step-1-verify-network-connectivity","title":"Step 1: Verify Network Connectivity","text":"<pre><code># Test if you can reach the Qumulo server\nping cfs15.cam.uchc.edu\n\n# Test if port 9000 is accessible\nnc -zv cfs15.cam.uchc.edu 9000\n# OR\ntelnet cfs15.cam.uchc.edu 9000\n</code></pre> <p>If these fail, you need to: - Connect to UCHC VPN - Ensure you're on the campus network</p>"},{"location":"QUMULO_SETUP/#step-2-test-with-aws-cli","title":"Step 2: Test with AWS CLI","text":"<p>Once connected to the network:</p> <pre><code># Set credentials as environment variables\nexport QUMULO_ACCESS_KEY=\"&lt;&lt;access-key-id&gt;&gt;\"\nexport QUMULO_SECRET=\"&lt;&lt;secret-key&gt;&gt;\"\n\n# List bucket contents\nAWS_ACCESS_KEY_ID=$QUMULO_ACCESS_KEY \\\nAWS_SECRET_ACCESS_KEY=$QUMULO_SECRET \\\naws s3 ls s3://sms-vivarium/ \\\n  --endpoint-url https://cfs15.cam.uchc.edu:9000 \\\n  --no-verify-ssl\n\n# Upload a test file (requires checksum settings for Qumulo compatibility)\necho \"Test content\" &gt; /tmp/test.txt\nAWS_REQUEST_CHECKSUM_CALCULATION=when_required \\\nAWS_RESPONSE_CHECKSUM_VALIDATION=when_required \\\nAWS_S3_ADDRESSING_STYLE=path \\\nAWS_DEFAULT_REGION=us-east-1 \\\nAWS_ACCESS_KEY_ID=$QUMULO_ACCESS_KEY \\\nAWS_SECRET_ACCESS_KEY=$QUMULO_SECRET \\\naws s3api put-object \\\n  --bucket sms-vivarium \\\n  --key test/aws_test.txt \\\n  --body /tmp/test.txt \\\n  --endpoint-url https://cfs15.cam.uchc.edu:9000 \\\n  --no-verify-ssl\n\n# Download the file\nAWS_ACCESS_KEY_ID=$QUMULO_ACCESS_KEY \\\nAWS_SECRET_ACCESS_KEY=$QUMULO_SECRET \\\naws s3 cp s3://sms-vivarium/test/aws_test.txt /tmp/downloaded.txt \\\n  --endpoint-url https://cfs15.cam.uchc.edu:9000 \\\n  --no-verify-ssl\n\n# Verify content\ncat /tmp/downloaded.txt\n</code></pre>"},{"location":"QUMULO_SETUP/#step-3-run-python-tests","title":"Step 3: Run Python Tests","text":"<pre><code># Switch to Qumulo backend\n# Edit assets/dev/config/.dev_env:\nSTORAGE_BACKEND=qumulo\n\n# Run the Qumulo integration tests\nuv run pytest tests/test_qumulo_s3.py -v -s\n\n# Or run a specific test\nuv run pytest tests/test_qumulo_s3.py::test_qumulo_file_service -v -s\n</code></pre>"},{"location":"QUMULO_SETUP/#using-qumulo-fileservice-in-your-code","title":"Using Qumulo FileService in Your Code","text":""},{"location":"QUMULO_SETUP/#option-1-via-configured-backend-recommended","title":"Option 1: Via Configured Backend (Recommended)","text":"<pre><code>from sms_api.dependencies import get_file_service\n\n# Get the configured file service (Qumulo when STORAGE_BACKEND=qumulo)\nfile_service = get_file_service()\n\n# Upload\nawait file_service.upload_file(\n    Path(\"local/file.txt\"),\n    \"simulations/results/data.txt\"  # Path-based, maps to Qumulo filesystem\n)\n\n# Download\ncontents = await file_service.get_file_contents(\"simulations/results/data.txt\")\n</code></pre>"},{"location":"QUMULO_SETUP/#option-2-direct-instantiation","title":"Option 2: Direct Instantiation","text":"<pre><code>from sms_api.common.storage import FileServiceQumuloS3\n\n# Create Qumulo service\nqumulo = FileServiceQumuloS3()\n\ntry:\n    # Upload bytes\n    await qumulo.upload_bytes(\n        b\"simulation data\",\n        \"experiments/exp001/output.bin\"\n    )\n\n    # List files\n    listing = await qumulo.get_listing(\"experiments/exp001/\")\n    for item in listing:\n        print(f\"{item.Key}: {item.Size} bytes\")\n\nfinally:\n    await qumulo.close()\n</code></pre>"},{"location":"QUMULO_SETUP/#option-3-in-tests-with-fixture","title":"Option 3: In Tests (with Fixture)","text":"<pre><code>import pytest\nfrom sms_api.common.storage import FileServiceQumuloS3\n\n@pytest.mark.asyncio\nasync def test_my_feature(file_service_qumulo: FileServiceQumuloS3):\n    # Service is already configured and injected!\n    result = await file_service_qumulo.upload_bytes(\n        b\"test data\",\n        \"test/my_test.bin\"\n    )\n    # Automatic cleanup via fixture\n</code></pre>"},{"location":"QUMULO_SETUP/#path-format-for-qumulo","title":"Path Format for Qumulo","text":"<p>Qumulo uses path-based bucket access. All these formats work:</p> <pre><code># Relative path (recommended)\n\"simulations/exp001/results.parquet\"\n\n# Absolute-style path\n\"/simulations/exp001/results.parquet\"\n\n# With protocol prefix\n\"qumulo://simulations/exp001/results.parquet\"\n</code></pre> <p>The bucket (<code>sms-vivarium</code>) is automatically prepended, so the full S3 path becomes:</p> <pre><code>s3://sms-vivarium/simulations/exp001/results.parquet\n</code></pre>"},{"location":"QUMULO_SETUP/#troubleshooting","title":"Troubleshooting","text":""},{"location":"QUMULO_SETUP/#connection-timeouts","title":"Connection Timeouts","text":"<p>Symptom: Tests hang or timeout when connecting to Qumulo</p> <p>Solutions: 1. Verify you're connected to UCHC VPN 2. Check network connectivity: <code>ping cfs15.cam.uchc.edu</code> 3. Verify port access: <code>nc -zv cfs15.cam.uchc.edu 9000</code> 4. Check firewall rules on your machine</p>"},{"location":"QUMULO_SETUP/#authentication-errors","title":"Authentication Errors","text":"<p>Symptom: <code>403 Forbidden</code> or <code>Access Denied</code></p> <p>Solutions: 1. Verify credentials in <code>.dev_env</code> are correct 2. Check that credentials haven't expired 3. Ensure the access key has permissions for the <code>sms-vivarium</code> bucket</p>"},{"location":"QUMULO_SETUP/#ssltls-errors","title":"SSL/TLS Errors","text":"<p>Symptom: <code>SSL verification failed</code> or certificate errors</p> <p>Solutions: 1. Ensure <code>STORAGE_QUMULO_VERIFY_SSL=false</code> in <code>.dev_env</code> 2. If Qumulo is using HTTPS, update endpoint to <code>https://</code> and set verify to <code>true</code></p>"},{"location":"QUMULO_SETUP/#wrong-region-errors","title":"Wrong Region Errors","text":"<p>Note: Qumulo doesn't use AWS regions. The implementation ignores region settings.</p>"},{"location":"QUMULO_SETUP/#comparison-aws-s3-vs-qumulo-s3","title":"Comparison: AWS S3 vs Qumulo S3","text":"Feature AWS S3 Qumulo S3 Endpoint <code>s3.amazonaws.com</code> <code>cfs15.cam.uchc.edu:9000</code> Authentication IAM, SSO, Keys Access Key/Secret only SSL/TLS Required (HTTPS) Optional (HTTP) Regions Required N/A (ignored) Bucket Style Virtual-hosted Path-based Network Public internet Private network (VPN required)"},{"location":"QUMULO_SETUP/#production-deployment","title":"Production Deployment","text":"<p>When deploying to Kubernetes/HPC environments that have network access to Qumulo:</p> <pre><code># ConfigMap or Secret\napiVersion: v1\nkind: Secret\nmetadata:\n  name: storage-config\nstringData:\n  STORAGE_BACKEND: \"qumulo\"\n  STORAGE_QUMULO_ENDPOINT_URL: \"http://cfs15.cam.uchc.edu:9000\"\n  STORAGE_QUMULO_BUCKET: \"sms-vivarium\"\n  STORAGE_QUMULO_ACCESS_KEY_ID: \"&lt;your-key&gt;\"\n  STORAGE_QUMULO_SECRET_ACCESS_KEY: \"&lt;your-secret&gt;\"\n  STORAGE_QUMULO_VERIFY_SSL: \"false\"\n</code></pre>"},{"location":"QUMULO_SETUP/#next-steps","title":"Next Steps","text":"<p>Once network connectivity is established:</p> <ol> <li>Run basic connection tests with AWS CLI</li> <li>Run Python integration tests</li> <li>Upload/download test files</li> <li>Integrate with your simulation workflows</li> </ol>"},{"location":"QUMULO_SETUP/#support","title":"Support","text":"<p>If you encounter issues: - Check network connectivity to <code>cfs15.cam.uchc.edu</code> - Verify VPN connection to UCHC network - Contact UCHC IT for Qumulo access issues - Check Qumulo documentation for S3 API compatibility</p>"},{"location":"modules/","title":"Services","text":"<p>The <code>DatabaseService</code> is the persistence layer for the simulation service. It provides methods to interact with the database, such as saving and retrieving simulation data.</p> <p>               Bases: <code>ABC</code></p> Source code in <code>sms_api/simulation/database_service.py</code> <pre><code>class DatabaseService(ABC):\n    @abstractmethod\n    async def insert_analysis(\n        self, name: str, config: AnalysisConfig, last_updated: str, job_name: str, job_id: int\n    ) -&gt; ExperimentAnalysisDTO:\n        \"\"\"Used by the /ecoli router\"\"\"\n        pass\n\n    @abstractmethod\n    async def get_analysis(self, database_id: int) -&gt; ExperimentAnalysisDTO:\n        \"\"\"Used by the /ecoli router\"\"\"\n        pass\n\n    @abstractmethod\n    async def list_analyses(self) -&gt; list[ExperimentAnalysisDTO]:\n        \"\"\"Used by the /ecoli router\"\"\"\n        pass\n\n    ####################################\n\n    @abstractmethod\n    async def insert_worker_event(self, worker_event: WorkerEvent, hpcrun_id: int) -&gt; WorkerEvent:\n        pass\n\n    @abstractmethod\n    async def list_worker_events(self, hpcrun_id: int, prev_sequence_number: int | None = None) -&gt; list[WorkerEvent]:\n        pass\n\n    @abstractmethod\n    async def insert_simulator(self, git_commit_hash: str, git_repo_url: str, git_branch: str) -&gt; SimulatorVersion:\n        pass\n\n    @abstractmethod\n    async def get_simulator(self, simulator_id: int) -&gt; SimulatorVersion | None:\n        pass\n\n    @abstractmethod\n    async def get_simulator_by_commit(self, commit_hash: str) -&gt; SimulatorVersion | None:\n        pass\n\n    @abstractmethod\n    async def delete_simulator(self, simulator_id: int) -&gt; None:\n        pass\n\n    @abstractmethod\n    async def list_simulators(self) -&gt; list[SimulatorVersion]:\n        pass\n\n    @abstractmethod\n    async def insert_hpcrun(self, slurmjobid: int, job_type: JobType, ref_id: int, correlation_id: str) -&gt; HpcRun:\n        \"\"\"\n        :param slurmjobid: (`int`) slurm job id for the associated `job_type`.\n        :param job_type: (`JobType`) job type to be run. Choose one of the following:\n            `JobType.SIMULATION`(/vecoli/run), `JobType.PARCA`(/vecoli/parca), `JobType.BUILD_IMAGE`(/simulator/new)\n        :param ref_id: primary key of the object this HPC run is associated with (sim, parca, etc.).\n        \"\"\"\n        pass\n\n    @abstractmethod\n    async def get_hpcrun_by_ref(self, ref_id: int, job_type: JobType) -&gt; HpcRun | None:\n        pass\n\n    @abstractmethod\n    async def get_hpcrun_by_slurmjobid(self, slurmjobid: int) -&gt; HpcRun | None:\n        pass\n\n    @abstractmethod\n    async def get_hpcrun(self, hpcrun_id: int) -&gt; HpcRun | None:\n        pass\n\n    @abstractmethod\n    async def get_hpcrun_id_by_correlation_id(self, correlation_id: str) -&gt; int | None:\n        pass\n\n    @abstractmethod\n    async def delete_hpcrun(self, hpcrun_id: int) -&gt; None:\n        pass\n\n    @abstractmethod\n    async def insert_parca_dataset(self, parca_dataset_request: ParcaDatasetRequest) -&gt; ParcaDataset:\n        pass\n\n    @abstractmethod\n    async def get_parca_dataset(self, parca_dataset_id: int) -&gt; ParcaDataset | None:\n        pass\n\n    @abstractmethod\n    async def delete_parca_dataset(self, parca_dataset_id: int) -&gt; None:\n        pass\n\n    @abstractmethod\n    async def list_parca_datasets(self) -&gt; list[ParcaDataset]:\n        pass\n\n    @abstractmethod\n    async def insert_simulation(self, sim_request: SimulationRequest) -&gt; Simulation:\n        pass\n\n    @abstractmethod\n    async def get_simulation(self, simulation_id: int) -&gt; Simulation | None:\n        pass\n\n    @abstractmethod\n    async def get_simulation_by_experiment_id(self, experiment_id: str) -&gt; Simulation | None:\n        \"\"\"Look up a simulation by its experiment_id (stored in config JSON).\"\"\"\n        pass\n\n    @abstractmethod\n    async def delete_simulation(self, simulation_id: int) -&gt; None:\n        pass\n\n    @abstractmethod\n    async def list_simulations(self) -&gt; list[Simulation]:\n        pass\n\n    @abstractmethod\n    async def list_active_hpcruns(self) -&gt; list[HpcRun]:\n        \"\"\"Return all HpcRun jobs with status PENDING or RUNNING.\"\"\"\n        pass\n\n    @abstractmethod\n    async def update_hpcrun_status(self, hpcrun_id: int, new_slurm_job: SlurmJob) -&gt; None:\n        \"\"\"Update the status of a given HpcRun job.\"\"\"\n        pass\n\n    @abstractmethod\n    async def close(self) -&gt; None:\n        pass\n</code></pre> <p>The <code>SimulationService</code> is the core service that handles the simulation logic. It interacts with the database service to save and retrieve simulation data, and it manages the execution of simulations on high-performance computing (HPC) resources.</p> <p>               Bases: <code>ABC</code></p> Source code in <code>sms_api/simulation/simulation_service.py</code> <pre><code>class SimulationService(ABC):\n    @abstractmethod\n    async def get_latest_commit_hash(\n        self,\n        git_repo_url: str = DEFAULT_REPO,\n        git_branch: str = DEFAULT_BRANCH,\n    ) -&gt; str:\n        pass\n\n    @abstractmethod\n    async def submit_build_image_job(self, simulator_version: SimulatorVersion, ssh: SSHSession) -&gt; int:\n        pass\n\n    @abstractmethod\n    async def submit_parca_job(self, parca_dataset: ParcaDataset, ssh: SSHSession) -&gt; int:\n        pass\n\n    @abstractmethod\n    async def submit_ecoli_simulation_job(\n        self, ecoli_simulation: Simulation, database_service: DatabaseService, correlation_id: str, ssh: SSHSession\n    ) -&gt; int:\n        pass\n\n    @abstractmethod\n    async def get_slurm_job_status(self, slurmjobid: int, ssh: SSHSession) -&gt; SlurmJob | None:\n        pass\n\n    @abstractmethod\n    async def close(self) -&gt; None:\n        pass\n</code></pre>"},{"location":"modules/#sms_api.simulation.database_service.DatabaseService.get_analysis","title":"<code>get_analysis(database_id)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Used by the /ecoli router</p> Source code in <code>sms_api/simulation/database_service.py</code> <pre><code>@abstractmethod\nasync def get_analysis(self, database_id: int) -&gt; ExperimentAnalysisDTO:\n    \"\"\"Used by the /ecoli router\"\"\"\n    pass\n</code></pre>"},{"location":"modules/#sms_api.simulation.database_service.DatabaseService.get_simulation_by_experiment_id","title":"<code>get_simulation_by_experiment_id(experiment_id)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Look up a simulation by its experiment_id (stored in config JSON).</p> Source code in <code>sms_api/simulation/database_service.py</code> <pre><code>@abstractmethod\nasync def get_simulation_by_experiment_id(self, experiment_id: str) -&gt; Simulation | None:\n    \"\"\"Look up a simulation by its experiment_id (stored in config JSON).\"\"\"\n    pass\n</code></pre>"},{"location":"modules/#sms_api.simulation.database_service.DatabaseService.insert_analysis","title":"<code>insert_analysis(name, config, last_updated, job_name, job_id)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Used by the /ecoli router</p> Source code in <code>sms_api/simulation/database_service.py</code> <pre><code>@abstractmethod\nasync def insert_analysis(\n    self, name: str, config: AnalysisConfig, last_updated: str, job_name: str, job_id: int\n) -&gt; ExperimentAnalysisDTO:\n    \"\"\"Used by the /ecoli router\"\"\"\n    pass\n</code></pre>"},{"location":"modules/#sms_api.simulation.database_service.DatabaseService.insert_hpcrun","title":"<code>insert_hpcrun(slurmjobid, job_type, ref_id, correlation_id)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>:param slurmjobid: (<code>int</code>) slurm job id for the associated <code>job_type</code>. :param job_type: (<code>JobType</code>) job type to be run. Choose one of the following:     <code>JobType.SIMULATION</code>(/vecoli/run), <code>JobType.PARCA</code>(/vecoli/parca), <code>JobType.BUILD_IMAGE</code>(/simulator/new) :param ref_id: primary key of the object this HPC run is associated with (sim, parca, etc.).</p> Source code in <code>sms_api/simulation/database_service.py</code> <pre><code>@abstractmethod\nasync def insert_hpcrun(self, slurmjobid: int, job_type: JobType, ref_id: int, correlation_id: str) -&gt; HpcRun:\n    \"\"\"\n    :param slurmjobid: (`int`) slurm job id for the associated `job_type`.\n    :param job_type: (`JobType`) job type to be run. Choose one of the following:\n        `JobType.SIMULATION`(/vecoli/run), `JobType.PARCA`(/vecoli/parca), `JobType.BUILD_IMAGE`(/simulator/new)\n    :param ref_id: primary key of the object this HPC run is associated with (sim, parca, etc.).\n    \"\"\"\n    pass\n</code></pre>"},{"location":"modules/#sms_api.simulation.database_service.DatabaseService.list_active_hpcruns","title":"<code>list_active_hpcruns()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Return all HpcRun jobs with status PENDING or RUNNING.</p> Source code in <code>sms_api/simulation/database_service.py</code> <pre><code>@abstractmethod\nasync def list_active_hpcruns(self) -&gt; list[HpcRun]:\n    \"\"\"Return all HpcRun jobs with status PENDING or RUNNING.\"\"\"\n    pass\n</code></pre>"},{"location":"modules/#sms_api.simulation.database_service.DatabaseService.list_analyses","title":"<code>list_analyses()</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Used by the /ecoli router</p> Source code in <code>sms_api/simulation/database_service.py</code> <pre><code>@abstractmethod\nasync def list_analyses(self) -&gt; list[ExperimentAnalysisDTO]:\n    \"\"\"Used by the /ecoli router\"\"\"\n    pass\n</code></pre>"},{"location":"modules/#sms_api.simulation.database_service.DatabaseService.update_hpcrun_status","title":"<code>update_hpcrun_status(hpcrun_id, new_slurm_job)</code>  <code>abstractmethod</code> <code>async</code>","text":"<p>Update the status of a given HpcRun job.</p> Source code in <code>sms_api/simulation/database_service.py</code> <pre><code>@abstractmethod\nasync def update_hpcrun_status(self, hpcrun_id: int, new_slurm_job: SlurmJob) -&gt; None:\n    \"\"\"Update the status of a given HpcRun job.\"\"\"\n    pass\n</code></pre>"}]}